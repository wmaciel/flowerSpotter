{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification using Densely Sampled SIFT Features\n",
    "\n",
    "A very useful local image descriptor is the Scale-Invariant Feature Transform (SIFT). SIFT features are invariant to scale, rotation, and intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gist\n",
    "import imageutils\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images into a Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1020, 128, 128, 3)\n",
      "['bluebell', 'buttercup', 'colts_foot', 'cowslip', 'crocus', 'daffodil', 'daisy', 'dandelion', 'fritillary', 'iris', 'lily_valley', 'pansy', 'snowdrop', 'sunflower', 'tigerlily', 'tulip', 'windflower']\n"
     ]
    }
   ],
   "source": [
    "base_dir = 'square_images128_dsift'\n",
    "(X_train, y_train, classes, class_dict) = imageutils.load_data(base_dir + '/train')\n",
    "(X_test, y_test, _, _) = imageutils.load_data(base_dir + '/test')\n",
    "image_height = X_train.shape[1]\n",
    "image_width = X_train.shape[2]\n",
    "print(X_train.shape)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute SIFT Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## loading codebook from codebook_dsift_10_5_300.file\n",
      "## compute the visual words histograms for each image\n",
      "(1020, 300)\n",
      "## compute the visual words histograms for each image\n",
      "(340, 300)\n",
      "train_sift_features: (1020, 300)\n",
      "test_sift_features: (340, 300)\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists, isdir, basename, join, splitext\n",
    "import sift\n",
    "from glob import glob\n",
    "from numpy import zeros, resize, sqrt, histogram, hstack, vstack, savetxt, zeros_like\n",
    "import scipy.cluster.vq as vq\n",
    "from cPickle import dump, load, HIGHEST_PROTOCOL\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "size = 10\n",
    "step = 5\n",
    "num_clusters = 300\n",
    "\n",
    "dataset_path = '../flower_rec1/square_images128_dsift'\n",
    "K_THRESH = 1\n",
    "codebook_file = \"codebook_dsift_{0}_{1}_{2}.file\".format(size, step, num_clusters)\n",
    "\n",
    "\n",
    "def get_categories(datasetpath):\n",
    "    cat_paths = [files for files in glob(datasetpath + \"/*\") if isdir(files)]\n",
    "    cat_paths.sort()\n",
    "    cats = [basename(cat_path) for cat_path in cat_paths]\n",
    "    return cats\n",
    "\n",
    "def get_sift_files(path):\n",
    "    all_files = []\n",
    "    all_files.extend([join(path, basename(fname)) for fname in glob(path + \"/*\") if splitext(fname)[-1].lower() == \".dsift_{0}_{1}\".format(size,step)])\n",
    "    return all_files\n",
    "\n",
    "def computeHistograms(codebook, descriptors):\n",
    "    code, dist = vq.vq(descriptors, codebook)\n",
    "    histogram_of_words, bin_edges = histogram(code, bins=range(codebook.shape[0] + 1), normed=True)\n",
    "    return histogram_of_words\n",
    "\n",
    "\n",
    "print \"## loading codebook from \" + codebook_file\n",
    "with open(codebook_file, 'rb') as f:\n",
    "    codebook = load(f)\n",
    "\n",
    "\n",
    "def sift_features(folder):\n",
    "    folder_path = dataset_path + '/' + folder\n",
    "    categories = get_categories(folder_path)\n",
    "\n",
    "    # Find the training SIFT files\n",
    "    all_sift_files = []\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = join(folder_path, category)\n",
    "        sift_file_list = get_sift_files(category_path)\n",
    "        all_sift_files += sift_file_list\n",
    "\n",
    "    all_sift_files = sorted(all_sift_files)\n",
    "\n",
    "\n",
    "    all_file_descriptors = dict()\n",
    "    sift_descriptors = []\n",
    "\n",
    "    for sift_file in all_sift_files:\n",
    "        desc = sift.read_features_from_file(sift_file)[1]\n",
    "        sift_descriptors.append(desc)\n",
    "        all_file_descriptors[sift_file] = desc\n",
    "\n",
    "\n",
    "    print \"## compute the visual words histograms for each image\"\n",
    "    all_word_histograms = dict()\n",
    "\n",
    "    sift_feature_rows = []\n",
    "\n",
    "    for sift_file in all_sift_files:\n",
    "        word_histogram = computeHistograms(codebook, all_file_descriptors[sift_file])\n",
    "        all_word_histograms[sift_file] = word_histogram\n",
    "        sift_feature_rows.append(word_histogram)\n",
    "\n",
    "    sift_feature_matrix = np.vstack(sift_feature_rows)\n",
    "    print(sift_feature_matrix.shape)\n",
    "    return sift_feature_matrix\n",
    "\n",
    "\n",
    "X_train_sift_features = sift_features('train')\n",
    "X_test_sift_features = sift_features('test')\n",
    "\n",
    "print(\"train_sift_features: {0}\".format(X_train_sift_features.shape))\n",
    "print(\"test_sift_features: {0}\".format(X_test_sift_features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Shuffle the Rows in the Train Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle the input images and labels (IN THE SAME RANDOM ORDER SO THEY ARE STILL CORRELATED)\n",
    "#rng_state = np.random.get_state()\n",
    "#np.random.shuffle(X_train_sift_features)\n",
    "#np.random.set_state(rng_state)\n",
    "#np.random.shuffle(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.288235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "svm = SVC(kernel='linear', class_weight='balanced')\n",
    "svm.fit(X_train_sift_features, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test_sift_features)\n",
    "print('accuracy: %f' % (np.mean(y_pred == y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "num_examples = X_train_sift_features.shape[0]\n",
    "cv = KFold(num_examples, n_folds=10, random_state=None)\n",
    "\n",
    "svm_cv_accuracies = cross_val_score(svm, X_train_sift_features, y_train, cv=cv, n_jobs=7)\n",
    "print(svm_cv_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try combining this with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_gist(I):\n",
    "    return gist.extract(I.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tiny_image(image_array, width=16, height=16):\n",
    "    I = Image.fromarray(image_array.astype('uint8'))\n",
    "    I_tiny = I.resize((width, height))\n",
    "    I_tiny_array = np.array(I_tiny).astype('float')\n",
    "    return np.reshape(I_tiny_array, width * height * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(X, feature_functions):\n",
    "    num_examples = X.shape[0]\n",
    "    feature_rows = []\n",
    "    for index in range(num_examples):\n",
    "        I = X[index]\n",
    "        features = []\n",
    "        for feature_func in feature_functions:\n",
    "            feature_vec = feature_func(I)\n",
    "            features.append(feature_vec)\n",
    "        \n",
    "        example_features_row_vec = np.hstack(features)\n",
    "        feature_rows.append(example_features_row_vec)\n",
    "    \n",
    "    X_features = np.vstack(feature_rows)\n",
    "    return X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1020, 2229)\n"
     ]
    }
   ],
   "source": [
    "from features import hog_feature, color_histogram_hsv\n",
    "\n",
    "num_color_bins = 200 # Number of bins in the color histogram\n",
    "feature_fns = [extract_gist, tiny_image, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n",
    "X_train_features = extract_features(X_train, feature_fns)\n",
    "X_test_features = extract_features(X_test, feature_fns)\n",
    "\n",
    "X_train_features = np.hstack([X_train_features, X_train_sift_features])\n",
    "X_test_features = np.hstack([X_test_features, X_test_sift_features])\n",
    "\n",
    "# Preprocessing: Subtract the mean feature\n",
    "mean_features = np.mean(X_train_features, axis=0)\n",
    "mean_features = np.expand_dims(mean_features, axis=0)\n",
    "X_train_features -= mean_features\n",
    "X_test_features -= mean_features\n",
    "\n",
    "# Preprocessing: Divide by standard deviation. This ensures that each feature\n",
    "# has roughly the same scale.\n",
    "std_features = np.std(X_train_features, axis=0)\n",
    "std_features = np.expand_dims(std_features, axis=0)\n",
    "X_train_features /= std_features\n",
    "X_test_features /= std_features\n",
    "\n",
    "# Preprocessing: Add a bias dimension\n",
    "X_train_features = np.hstack([X_train_features, np.ones((X_train_features.shape[0], 1))])\n",
    "X_test_features = np.hstack([X_test_features, np.ones((X_test_features.shape[0], 1))])\n",
    "\n",
    "print(X_train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.729412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "svm = SVC(kernel='linear', class_weight='balanced')\n",
    "svm.fit(X_train_features, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test_features)\n",
    "print('accuracy: %f' % (np.mean(y_pred == y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.67647059  0.65686275  0.68627451  0.66666667  0.66666667  0.61764706\n",
      "  0.59803922  0.66666667  0.81372549  0.61764706]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "num_examples = X_train_features.shape[0]\n",
    "cv = KFold(num_examples, n_folds=10, shuffle=True, random_state=None)\n",
    "\n",
    "svm_cv_accuracies = cross_val_score(svm, X_train_features, y_train, cv=cv, n_jobs=7)\n",
    "print(svm_cv_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71965952  0.00167979  0.00604713  0.00854004  0.00562125  0.00186928\n",
      "  0.01598482  0.00182104  0.04596984  0.05848008  0.03349536  0.03800551\n",
      "  0.02531016  0.00236164  0.00414568  0.01001119  0.02099767]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "svm = SVC(kernel='linear', class_weight='balanced', probability=True)\n",
    "svm.fit(X_train_features, y_train)\n",
    "\n",
    "y_pred = svm.predict_proba(X_test_features)\n",
    "print(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 9, 8, 11, 10]\n",
      "Top-5 accuracy: 0.947058823529\n",
      "Top-1 accuracy: 0.747058823529\n"
     ]
    }
   ],
   "source": [
    "y_augmented = [[(prob, index) for (index,prob) in enumerate(y_pred[index])] for index in range(len(y_pred))]\n",
    "y_sorted = [sorted(y_augmented[index], reverse=True) for index in range(len(y_augmented))]\n",
    "y_top5 = [y_sorted[index][:5] for index in range(len(y_sorted))]\n",
    "y_top5_labels = [[label for (prob, label) in y_top5[index]] for index in range(len(y_sorted))]\n",
    "print(y_top5_labels[0])\n",
    "\n",
    "num_within_top_5 = 0.0\n",
    "for index in range(len(y_test)):\n",
    "    if y_test[index] in y_top5_labels[index]:\n",
    "        num_within_top_5 += 1.0\n",
    "\n",
    "percentage_within_top_5 = num_within_top_5 / len(y_test)\n",
    "print(\"Top-5 accuracy: {0}\".format(percentage_within_top_5))\n",
    "\n",
    "y_top1_labels = [labels[0] for labels in y_top5_labels]\n",
    "print(\"Top-1 accuracy: {0}\".format((np.mean(y_top1_labels == y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "    'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "#clf = GridSearchCV(svm, param_grid, scoring='f1', cv=cv, n_jobs=7)\n",
    "clf = GridSearchCV(svm, param_grid, cv=cv, n_jobs=7)\n",
    "\n",
    "clf = clf.fit(X_train_features, y_train)\n",
    "\n",
    "print(\"Best estimator found by randomized hyper parameter search:\")\n",
    "print(clf.best_params_)\n",
    "print(\"Best parameters validation score: {:.3f}\".format(clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
